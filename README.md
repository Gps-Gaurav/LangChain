
# ğŸŒŸ LangChain

## ğŸ“Œ What is LangChain?  
LangChain is an **open-source framework** designed to make it easier to build **applications powered by Large Language Models (LLMs)**.  

Instead of writing raw model calls every time, LangChain gives you:  
- ğŸ”§ **Modular building blocks** (models, prompts, chains, memory, tools, agents)  
- ğŸ”— **Integrations** with external systems (databases, APIs, vector stores, web pages, PDFs)  
- ğŸ§  **High-level abstractions** to handle context, structured outputs, and reasoning  
- ğŸš€ **Production-ready features** like evaluation, monitoring, and scaling  

Think of LangChain as a **Swiss army knife for AI apps**.  

---

## ğŸš€ Why LangChain First?  
When building with LLMs, developers face challenges:  
- How do I **connect my app to data** (documents, databases, APIs)?  
- How do I manage **context, memory, and conversation history**?  
- How do I ensure **structured and reliable outputs**?  
- How do I combine multiple LLM calls in a workflow?  

LangChain solves these problems by offering:  
- **Model-agnostic development** â†’ Use OpenAI, Anthropic, HuggingFace, Google, or open-source models.  
- **Chains & Memory** â†’ Handle conversations, reasoning, and workflows.  
- **Tools & Agents** â†’ Let your AI interact with external systems (search, APIs, calculators).  
- **Retrieval-Augmented Generation (RAG)** â†’ Build smarter, grounded, knowledge-aware systems.  

---

## ğŸ§© Core Components of LangChain  

### 1ï¸âƒ£ Models  
The heart of LangChainâ€”how you talk to AI.  
- **LLMs** â†’ General-purpose text generation.  
- **Chat Models** â†’ Conversation-oriented (better than plain LLMs for dialogs).  
- **Embedding Models** â†’ Turn text into vectors for semantic search & retrieval.  

ğŸ‘‰ Example:  

```python
from langchain_openai import ChatOpenAI
model = ChatOpenAI(model="gpt-4o-mini")
print(model.invoke("Tell me a joke about AI."))
```

---

### 2ï¸âƒ£ Prompts  
Prompts guide the modelâ€™s output. LangChain makes them **dynamic, reusable, and structured**.  
- **Prompt Templates** â†’ Insert variables (`{topic}`) into pre-written prompts.  
- **Chat Prompt Templates** â†’ Multi-turn conversation templates.  
- **Message Placeholders** â†’ Automatically inject conversation history.  

ğŸ‘‰ Example:  

```python
from langchain.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a teacher."),
    ("user", "Explain {topic} like Iâ€™m 5.")
])

print(prompt.format(topic="machine learning"))
```

---

### 3ï¸âƒ£ Chains  
Chains are **pipelines** where outputs of one step feed into the next.  
- **Simple Chain** â†’ One prompt + one model.  
- **Sequential Chain** â†’ Step-by-step execution.  
- **Parallel Chain** â†’ Run multiple models at once.  
- **Conditional Chain** â†’ Different logic paths (like if/else).  

ğŸ‘‰ Example use case:  
1. Summarize a PDF â†’ 2. Translate into Hindi â†’ 3. Create a quiz.  

---

### 4ï¸âƒ£ Memory  
LLMs are **stateless** (they forget after each call). LangChain adds memory:  
- **Buffer Memory** â†’ Stores full conversation.  
- **Window Memory** â†’ Keeps only last N messages.  
- **Summarizer Memory** â†’ Compresses history into summaries.  
- **Custom Memory** â†’ Store user preferences, facts, etc.  

This makes chatbots feel **personal and continuous**.  

---

### 5ï¸âƒ£ Structured Output & Parsers  
By default, LLMs return text. But apps often need **JSON, CSV, or typed data**.  

LangChain offers:  
- **TypedDicts** â†’ Define strict schemas.  
- **Pydantic Models** â†’ Validate data structures.  
- **Output Parsers** â†’ Convert raw text â†’ structured formats.  

ğŸ‘‰ Example: Extracting structured reviews:  

```python
from langchain.output_parsers import StructuredOutputParser, ResponseSchema

schema = [ResponseSchema(name="summary", type="string"),
          ResponseSchema(name="sentiment", type="string")]
parser = StructuredOutputParser.from_response_schemas(schema)

output = parser.parse('{"summary": "Great product", "sentiment": "positive"}')
print(output)
```

---

### 6ï¸âƒ£ Runnables & LCEL (LangChain Expression Language)  
Instead of manually wiring components, LCEL lets you **compose workflows elegantly**.  

- **RunnableSequence** â†’ Sequential steps  
- **RunnableParallel** â†’ Run multiple tasks together  
- **RunnableLambda** â†’ Custom Python functions in workflow  
- **RunnableBranch** â†’ Conditional routing  

ğŸ‘‰ Example (pseudocode):  

```python
chain = prompt | model | parser
response = chain.invoke({"input": "What is LangChain?"})
```

---

### 7ï¸âƒ£ Document Loaders & Text Splitters  
Load data from **PDFs, websites, CSVs, text files, directories**.  
- `PyPDFLoader` â†’ Extract text from PDFs.  
- `WebBaseLoader` â†’ Scrape websites.  
- `CSVLoader` â†’ Convert CSV rows into LangChain Documents.  

Large documents are split into **chunks** for efficient retrieval.  

---

### 8ï¸âƒ£ Vector Stores & Retrievers  
Used for **semantic search** and **RAG**.  
- Vector Stores â†’ FAISS, Pinecone, Chroma, Weaviate, Qdrant.  
- Retrievers â†’ Search for relevant docs. Includes:  
  - **VectorStoreRetriever**  
  - **Multi-Query Retriever**  
  - **Contextual Compression Retriever**  

---

### 9ï¸âƒ£ RAG (Retrieval-Augmented Generation)  
RAG = **LLM + knowledge base**.  

Steps:  
1. **Indexing** â†’ Load â†’ Chunk â†’ Embed â†’ Store in vector DB.  
2. **Retrieval** â†’ Find top relevant chunks.  
3. **Augmentation** â†’ Merge with user query.  
4. **Generation** â†’ LLM produces grounded answers.  

ğŸ‘‰ Benefits:  
- Uses **up-to-date data**  
- No model retraining needed  
- Handles **large document collections**  

---

### ğŸ”Ÿ Tools & Agents  
- **Tools** â†’ Functions/APIs that an agent can call. (E.g., calculator, search, currency converter)  
- **Agents** â†’ LLM-powered systems that **decide when & how to use tools**.  
- Popular Agent design â†’ **ReAct (Reason + Act)** â†’ LLM thinks step by step, calls tools, then answers.  

ğŸ‘‰ Example:  
An agent that answers: *â€œWhatâ€™s the population of Japan + 10%?â€*  
1. Searches population (via API)  
2. Calls calculator tool  
3. Returns final answer  

---

## âš¡ Installation  

```bash
pip install langchain
pip install langchain-community   # integrations
pip install langchain-openai      # OpenAI support
pip install chromadb              # example vector store
```

---

## ğŸ“– Quick Start Example  

```python
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate

# Model
model = ChatOpenAI(model="gpt-4o-mini")

# Prompt
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful AI assistant."),
    ("user", "{question}")
])

# Chain
chain = prompt | model
response = chain.invoke({"question": "Explain RAG in 2 lines."})

print(response)
```

---

## ğŸ”— Alternatives  
- [LlamaIndex](https://www.llamaindex.ai/) â†’ Best for knowledge indexing  
- [Haystack](https://haystack.deepset.ai/) â†’ Production search & RAG pipelines  

---

## ğŸ“š References  
- [LangChain Docs](https://docs.langchain.com)  
- [LangSmith](https://smith.langchain.com) â†’ Monitoring & Evaluation  

---

âœ¨ **LangChain = Build AI apps faster.**  
With components for models, prompts, chains, memory, retrieval, and agents, you can go from **prototype â†’ production** without reinventing the wheel.  
